{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ff70e1-4a8c-4681-aac0-cfed633ead62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515d9091-91c4-4f69-9d79-9a6d267db3e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Some helper functions for procesing the features from the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16db3c74-b640-4b18-9506-d144730020c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encode(df):\n",
    "    \"\"\"Encode categorical variables.\"\"\"\n",
    "\n",
    "    ordered_categories = {\n",
    "        'Home Ownership': ['MORTGAGE', 'RENT', 'OWN'],\n",
    "        'Income Verification Status': ['Not Verified', 'Partially Verified', 'Verified']\n",
    "    }\n",
    "    unordered_categories = ['State', 'Loan Purpose', 'Due Settlement', 'Payment Plan']\n",
    "\n",
    "    # Nominal categories\n",
    "    for name in unordered_categories:\n",
    "        df[name] = df[name].astype(\"category\")\n",
    "    # Ordinal categories\n",
    "    for name, levels in ordered_categories.items():\n",
    "        df[name] = df[name].astype(CategoricalDtype(levels,\n",
    "                                                    ordered=True))\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_table_and_encode(table_name):\n",
    "    spk_df = spark.read.table(table_name)\n",
    "    df = spk_df.toPandas()\n",
    "    return encode(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a510e86-f0ea-477c-ab37-f36e98315632",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Loading the data and creating the pipeline for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1249ef-9d9d-495f-9a62-b7d78873398b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "encoded_test = read_table_and_encode(\"Test\")\n",
    "encoded_train = read_table_and_encode(\"Train\")\n",
    "\n",
    "y_test = encoded_test.pop('Approve Loan')\n",
    "X_test = encoded_test.drop(columns=['id'])\n",
    "\n",
    "y_train = encoded_train.pop('Approve Loan')\n",
    "X_train = encoded_train.drop(columns=['id'])\n",
    "\n",
    "categorical_vars = X_train.dtypes[X_train.dtypes == 'category'].index.to_list()\n",
    "numerical_vars = [col for col in X_train.columns if col not in categorical_vars]\n",
    "\n",
    "numeric_preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"categorical\", categorical_preprocessor, categorical_vars),\n",
    "        (\"numerical\", numeric_preprocessor, numerical_vars),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f28121a-dde1-448f-8eda-bc87aceadb4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create the preprocessing and training pipeline and run the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385a99c3-079a-44ae-b27d-08516356d184",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/30 21:54:23 WARNING mlflow.utils: Truncated the value of the key `steps`. Truncated value: `[('columntransformer', ColumnTransformer(transformers=[('categorical',\n",
      "                                 Pipeline(steps=[('onehot',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                 ['State', 'Income Verification Status',\n",
      "                                  'Home Ownership', 'Loan Purpose',\n",
      "                                  'Due Settlement', 'Payment Plan']),\n",
      "                                ('numerical',\n",
      "                ...`\n",
      "2023/11/30 21:54:23 WARNING mlflow.utils: Truncated the value of the key `columntransformer`. Truncated value: `ColumnTransformer(transformers=[('categorical',\n",
      "                                 Pipeline(steps=[('onehot',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                 ['State', 'Income Verification Status',\n",
      "                                  'Home Ownership', 'Loan Purpose',\n",
      "                                  'Due Settlement', 'Payment Plan']),\n",
      "                                ('numerical',\n",
      "                                 Pipeli...`\n",
      "2023/11/30 21:54:23 WARNING mlflow.data.pandas_dataset: Failed to infer schema for Pandas dataset. Exception: Unable to map 'object' type to MLflow DataType. object can be mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\n",
      "2023/11/30 21:54:24 WARNING mlflow.sklearn: Failed to infer model signature: Unable to map 'object' type to MLflow DataType. object can be mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(preprocessor, LogisticRegression())\n",
    "with mlflow.start_run():\n",
    "    pipe.fit(X_train, y_train).score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Feature processing and model training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
