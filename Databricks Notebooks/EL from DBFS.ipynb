{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5a3db8-87f8-41c5-949d-05040a6fd832",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading the data into Delta Tables \n",
    "\n",
    "- The test and train file were uploaded to the file store manually.\n",
    "- The data could be stored on the Feature Store, being stored with the ids as primary index. But since I'm using the community edition, the ingestion of the features for some of the features wouldn't be available for the prediction offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_csv_data_from_dbfs(file_location: str, table: str):\n",
    "    file_type = \"csv\"\n",
    "    infer_schema = \"true\"\n",
    "    first_row_is_header = \"true\"\n",
    "    delimiter = \",\"\n",
    "\n",
    "    df = spark.read.format(file_type) \\\n",
    "      .option(\"inferSchema\", infer_schema) \\\n",
    "      .option(\"header\", first_row_is_header) \\\n",
    "      .option(\"sep\", delimiter) \\\n",
    "      .load(file_location)\n",
    "\n",
    "    # Saving in delta and use column mapping to keep the original column names\n",
    "    df.write.format(\"delta\").option('delta.columnMapping.mode', 'name').mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692fc423-23f6-478a-90bc-bb71ac1b7ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_file_location = \"/FileStore/tables/Test.csv\"\n",
    "test_table = \"Test\"\n",
    "load_csv_data_from_dbfs(test_file_location, test_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd421a0b-038e-48eb-a80e-a4bd770db709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_file_location = \"/FileStore/tables/Train.csv\"\n",
    "train_table = \"Train\"\n",
    "load_csv_data_from_dbfs(train_file_location, train_table)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "EL from DBFS",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
